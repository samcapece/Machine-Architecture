#! /usr/bin/env python3

# Author: John Kolb <jhkolb@umn.edu>
# Inspired by Chris Kauffman's original 'testy' utility
# SPDX-License-Identifier: GPL-3.0-or-later
# Version 0.1.2
# Requires Python 3.6 or above
# Tested in Linux environments only

BUF_SIZE = 4096
DRAIN_OUTPUT_DELAY_SEC = 0.1
PARENT_PROC_DELAY_SEC = 0.5
DEFAULT_COMMAND = "bash --norc"
BASH_PROMPT = "$ "
DEFAULT_POINT_VALUE = 1
DEFAULT_TIMEOUT = 10
TEST_RESULTS_DIR = "test_results"
VALGRIND_ERROR_RET = 13
DEFAULT_VALGRIND_OPTS = f"--leak-check=full --show-leak-kinds=all --error-exitcode={VALGRIND_ERROR_RET} --track-origins=yes"
TERMIOS_LFLAG = 3
TERMIOS_CC = 6

import argparse
import contextlib
import difflib
import enum
import json
import os
import os.path
import pty
import re
import select
import shlex
import shutil
import signal
import subprocess
import sys
import termios
import time

class CommandOutcome(enum.Enum):
    COMPLETED = enum.auto()
    TIMED_OUT = enum.auto()
    SEG_FAULT = enum.auto()
    VALGRIND_FAIL = enum.auto()


# Simplifies logic for writing output to a file and optionally to stdout
class MultiWriter:
    def __init__(self, output_file, to_stdout=False):
        self.output_file = open(output_file, "w")
        self.to_stdout = to_stdout

    def write(self, s):
        if self.to_stdout:
            print(s, end="")
        self.output_file.write(s)

    def close(self):
        self.output_file.close()


# Expands lines of templated text in input/output files to the output of a shell command
# Example: {{pwd}}/foo/bar -> /home/goldy/csci4061/labs01-code/foo/bar
# Deals with either a list of lines or one big string
def _shellExpand(match):
    command = match.group(1)
    res = subprocess.run(command, shell=True, capture_output=True, text=True, check=True)
    stdout = res.stdout.strip()
    stderr = res.stderr.strip()
    if len(stderr) > 0:
        raise Exception(f"Template expansion command {command} failed. stderr: {stderr}")
    else:
        return stdout
def expandTemplate(template):
    if isinstance(template, list):
        return [ re.sub(r'{{(.+?)}}', _shellExpand, line) for line in template ]
    else:
        return re.sub(r'{{(.+?)}}', _shellExpand, template)


# Collects output from a pseudoterminal into a string, either until a timeout
# expires or until a new prompt appears (if applicable)
# fd: File descriptor for pseudoterminal master
# timeout: Maximum amount of time to wait in total, in seconds
# prompt: Command prompt emitted by an interactive program when it is ready to
#         accept a new command, or 'None' if no prompt is expected
# Returns: A triple consisting of pty's output (string), total time consumed,
#          and a boolean indicating if program is still alive
def drainOutput(fd, timeout, prompt):
    if prompt is not None:
        prompt = prompt.rstrip() # Ignore any trailing whitespace in prompt
    output = ""
    consumed_time = 0
    p = select.poll()
    p.register(fd, select.POLLIN)

    while (prompt is None or not output.rstrip().endswith(prompt)) and consumed_time < timeout:
        poll_start = time.perf_counter()
        res = p.poll((timeout - consumed_time) * 1000) # poll timeout is in msec rather than sec
        poll_end = time.perf_counter()
        consumed_time += (poll_end - poll_start)

        if len(res) > 0:
            _, revents = res[0]
            if revents & select.POLLIN:
                payload = os.read(fd, BUF_SIZE).decode("utf8")
                payload = payload.replace("\r\n", "\n")
                output += payload
            elif revents & select.POLLHUP:
                return output, consumed_time, False
        # else: poll() timed out, loop will end

    return output, consumed_time, True


# Execute a command and collect its output
# command: The command to run, including arguments, e.g., "ls -l"
# input_file: A file containing input for the command's stdin, or None if no input
# prompt: Command prompt emitted by the program when it is ready to accept new input
#         or 'None' if no prompt is used
# environment: A dictionary containing environment variables to be set for the program
# timeout: Maximum amount of time to wait for the command to terminate, in seconds
# use_valgrind: A boolean indicating whether or not to run the command through valgrind
# valgrind_opts: Extra options for valgrind, or 'None' if not needed
def executeCommand(command, input_file, prompt, timeout, environment, use_valgrind, valgrind_opts):
    args = shlex.split(command)
    command = args[0]
    pid, master_fd = pty.fork()
    if pid == 0:
        if use_valgrind:
            command = "valgrind"
            args = ["valgrind"] + shlex.split(valgrind_opts) + args

        if environment is None:
            os.execlp(command, *args)
        else:
            current_env = os.environ
            current_env.update(environment)
            os.execlpe(command, *args, current_env)

    # Parent process only
    try:
        time.sleep(PARENT_PROC_DELAY_SEC)
        # Make sure we can deliver signals via pty master
        term_attr = termios.tcgetattr(master_fd)
        term_attr[TERMIOS_LFLAG] |= termios.ISIG
        termios.tcsetattr(master_fd, termios.TCSANOW, term_attr)

        if input_file is None:
            output, total_consumed_time, _ = drainOutput(master_fd, timeout, prompt)
        else:
            output = ""
            total_consumed_time = 0
            with open(input_file) as input:
                input_lines = expandTemplate(input.readlines())
            i = 0

            if prompt is not None:
                # Wait as long as needed until first prompt appears
                delay = timeout
            else:
                # Wait for a bit, but no need to wait until a prompt appears
                delay = DRAIN_OUTPUT_DELAY_SEC
            output_batch, consumed_time, still_alive = drainOutput(master_fd, delay, prompt)
            output += output_batch
            total_consumed_time += consumed_time

            while i < len(input_lines) and still_alive and total_consumed_time < timeout:
                should_echo = True

                if input_lines[i] == '^C\n':
                    payload = term_attr[TERMIOS_CC][termios.VINTR]
                    should_echo = False
                elif input_lines[i] == '^Z\n':
                    payload = term_attr[TERMIOS_CC][termios.VSUSP]
                    should_echo = False
                elif input_lines[i] == '^D\n':
                    payload = term_attr[TERMIOS_CC][termios.VEOF]
                    should_echo = False
                elif prompt is not None and input_lines[i].startswith(prompt):
                    payload = input_lines[i][len(prompt):].lstrip().encode("utf8")
                else:
                    payload = input_lines[i].encode("utf8")

                if i == len(input_lines) - 1:
                    # Last line of input, no need to wait for next prompt
                    drain_prompt = None
                    delay = timeout - total_consumed_time
                elif prompt is not None and input_lines[i+1].startswith(prompt):
                    # Do not proceed until program outputs expected prompt
                    drain_prompt = prompt
                    delay = timeout - total_consumed_time
                else:
                    # Pause to collect output but do not await appearance of prompt
                    drain_prompt = None
                    delay = DRAIN_OUTPUT_DELAY_SEC

                if not should_echo:
                    term_attr[TERMIOS_LFLAG] &= ~termios.ECHO
                    termios.tcsetattr(master_fd, termios.TCSANOW, term_attr)

                os.write(master_fd, payload)
                output_batch, consumed_time, still_alive = drainOutput(master_fd, delay, drain_prompt)
                output += output_batch
                total_consumed_time += consumed_time
                i += 1

                if not should_echo:
                    # Restore echo as default for next line of input
                    term_attr[TERMIOS_LFLAG] |= termios.ECHO
                    termios.tcsetattr(master_fd, termios.TCSANOW, term_attr)

        timed_out = (total_consumed_time >= timeout)
        if timed_out:
            try:
                os.kill(pid, signal.SIGKILL)
            except ProcessLookupError:
                pass # Process terminated after timeout expired and before signal sent
        _, exit_status = os.waitpid(pid, 0)
        if timed_out:
            return output, CommandOutcome.TIMED_OUT
        elif os.WIFSIGNALED(exit_status) and os.WTERMSIG(exit_status) == signal.SIGSEGV:
            return output, CommandOutcome.SEG_FAULT
        elif use_valgrind and os.WIFEXITED(exit_status) and os.WEXITSTATUS(exit_status) == VALGRIND_ERROR_RET:
            return output, CommandOutcome.VALGRIND_FAIL
        return output, CommandOutcome.COMPLETED
    finally:
        os.close(master_fd)


# Validates test suite specification for correctness
def isValidTestSuite(test_suite):
    try:
        if "name" not in test_suite:
            print('Test suite missing "name" field')
            return False

        if "tests" not in test_suite:
            print('Test suite missing "tests" field')
            return False

        global_timeout = test_suite.get("timeout")
        if global_timeout is not None:
            try:
                int(global_timeout)
            except ValueError:
                print(f'Test suite specifies invalid timeout value "{global_timeout}"')
                return False

        global_environment = test_suite.get("environment")
        if global_environment is not None and not isinstance(global_environment, dict):
            print('Test suite has non-dictionary "environment" value')
            return False

        for i, test in enumerate(test_suite["tests"]):
            name = test.get("name")
            if name is None:
                print(f'Test {i+1} missing "name" field')
                return False

            description = test.get("description")
            if description is None:
                print(f'Test {i+1} missing "description" field')
                return False

            output_file = test.get("output_file")
            if output_file is None:
                print(f'Test {i+1} missing "output_file" field')
                return False
            elif not os.path.isfile(output_file):
                print(f'Test {i+1} output file "{output_file}" does not exist or is invalid')
                return False

            command = test.get("command", test_suite.get("command"))
            input_file = test.get("input_file")
            prompt = test.get("prompt", test_suite.get("prompt"))
            if command is None and input_file is None:
                print(f'Test {i+1} will default to bash but has no "input_file" field')
                return False
            if input_file is not None and not os.path.isfile(input_file):
                print(f'Test {i+1} input file "{input_file}" does not exist or is invalid')
                return False
            if input_file is None and prompt is not None:
                print(f'Test {i+1} specifies prompt but has no input file')
                return False

            point_value = test.get("points")
            if point_value is not None:
                try:
                    int(point_value)
                except ValueError:
                    print(f'Test {i+1} has invalid point value "{point_value}"')
                    return False

            timeout = test.get("timeout")
            if timeout is not None:
                try:
                    int(timeout)
                except ValueError:
                    print(f'Test {i+1} specifies invalid timeout duration "{timeout}"')
                    return False

            environment = test.get("environment")
            if environment is not None and not isinstance(environment, dict):
                print(f'Test {i+1} has non-dictionary "environment" value')
                return False

            use_valgrind = "use_valgrind" in test or "use_valgrind" in test_suite
            valgrind_opts = test.get("valgrind_opts", test_suite.get("valgrind_opts"))
            if valgrind_opts is not None and not use_valgrind:
                print(f'Test {i+1} specifies valgrind options but valgrind not enabled for this test')
                return False

        return True
    except:
        # Most likely means a value is not a dict or list as expected
        print("Test suite JSON is not valid data type")
        return False


# Simple way to compute number of digits in number 'n'
# Used to cleanly format output presented to user
def numDigits(n):
    digits = 0
    while n > 0:
        n = n // 10
        digits += 1
    return digits


# Compares expected and actual output line by line and generates a diff
# Note that empty lines and whitespace differences between tokens on a specific line are ignored
# expected_output: The output a program is expected to produce, as a string
# actual_output: The output generated by the program, as a string
# Returns a string containing a diff (both side-by-side comparison and a summary)
def compareOutput(expected_output, actual_output):
    expected_lines = [ line for line in re.split(r'\n+', expected_output) if len(line.strip()) > 0 ]
    actual_lines = [ line for line in re.split(r'\n+', actual_output) if len(line.strip()) > 0 ]
    expected_lines_trimmed = [ re.sub(r'\s+', ' ', line).strip() for line in expected_lines ]
    actual_lines_trimmed = [ re.sub(r'\s+', ' ', line).strip() for line in actual_lines]

    matcher = difflib.SequenceMatcher(lambda l: False, expected_lines_trimmed, actual_lines_trimmed)
    codes = matcher.get_opcodes()

    if len(codes) == 1 and codes[0][0] == 'equal':
        return True, ""

    # Used to justify output for clean formatting
    max_expected_line_len = max(max([ len(line) for line in expected_lines ]), len("==== EXPECT ===="))
    max_digits = max(numDigits(len(actual_lines)), numDigits(len(expected_lines)))

    comparison = f"{'==== EXPECT ====' : <{max_expected_line_len}}   ==== ACTUAL ====\n"
    summary = ""
    for tag, i1, i2, j1, j2 in codes:
        if tag == 'equal':
            for i, j in zip(range(i1, i2), range(j1, j2)):
                comparison += f"{expected_lines[i] : <{max_expected_line_len}}   {actual_lines[j]}\n"

        elif tag == 'delete':
            for i in range(i1, i2):
                comparison += f"{expected_lines[i] : <{max_expected_line_len}} <\n"
                summary += f"EXPECT {i+1 : >{max_digits}}) {expected_lines[i]}\n"

        elif tag == 'insert':
            for j in range(j1, j2):
                comparison += f"{' ' * max_expected_line_len} > {actual_lines[j]}\n"
                summary += f"ACTUAL {j+1 : >{max_digits}}) {actual_lines[j]}\n"

        elif tag == 'replace':
            # Replace is slightly unhelpful -- chunks identified from actual and expected can be different sizes
            # Print lines side-by-side until at least one chunk is exhausted
            for i, j in zip(range(i1, i2), range(j1, j2)):
                comparison += f"{expected_lines[i] : <{max_expected_line_len}} | {actual_lines[j]}\n"
                summary += f"EXPECT {i+1 : >{max_digits}}) {expected_lines[i]}\n" + \
                           f"ACTUAL {j+1 : >{max_digits}}) {actual_lines[j]}\n"

            expected_chunk_len = i2 - i1
            actual_chunk_len = j2 - j1
            if expected_chunk_len > actual_chunk_len:
                # Expected chunk is longer, so print out rest of its lines
                for i in range(i1 + actual_chunk_len, i2):
                    comparison += f"{expected_lines[i] : <{max_expected_line_len}} <\n"
                    summary += f"EXPECT {i+1 : >{max_digits}}) {expected_lines[i]}\n"
            elif actual_chunk_len > expected_chunk_len:
                # Actual chunk is longer, so print out rest of its lines
                for j in range(j1 + expected_chunk_len, j2):
                    comparison += f"{' ' * max_expected_line_len} > {actual_lines[j]}\n"
                    summary += f"ACTUAL {j+1 : >{max_digits}}) {actual_lines[j]}\n"

        else:
            raise ValueError(f"Unknown diff tag {tag}")

    full_diff = "== Side-by-Side Comparison ==\n" + \
            "== Differing lines have a character like '|' '>' or '<' in the middle\n" + \
            comparison + "== Line Differences ==\n" + summary.rstrip()
    return False, full_diff


# Used to generate file name based on a test's name
def stringToFileName(s):
    return re.sub(r'\s+', '_', s.lower())


# Output one-line summary of a test's outcome
def printTestSummaryLine(test_results, test_num_width):
    print(f"Test {test_results['index'] :>{test_num_width}}) {test_results['name']}: {test_results['outcome']}")


# Run a single test. Collects program output and generates a summary of the outcome
# test_num: The index (within the larger test suite) of the test to run. Starts at 1, not 0
# test_suite: Dictionary containing test suite's specification
# json_output: Boolean indicating whether or not JSON output is requested
# verbose_output: Boolean indicating whether or not verbose output is requested
# Returns a dictionary summarizing test's outcome
def runTest(test_num, test_suite, json_output, verbose_output):
    test_spec = test_suite["tests"][test_num - 1]
    command = test_spec.get("command", test_suite.get("command", DEFAULT_COMMAND))
    output_file = test_spec["output_file"]
    input_file = test_spec.get("input_file")
    prompt = test_spec.get("prompt", test_suite.get("prompt"))
    max_score = test_spec.get("points", DEFAULT_POINT_VALUE)
    timeout = int(test_spec.get("timeout", test_suite.get("timeout", DEFAULT_TIMEOUT)))
    environment = test_spec.get("environment", test_suite.get("environment"))
    use_valgrind = ("use_valgrind" in test_spec) or ("use_valgrind" in test_suite)
    valgrind_opts = test_spec.get("valgrind_opts", test_suite.get("valgrind_opts", DEFAULT_VALGRIND_OPTS))
    if command == DEFAULT_COMMAND:
        prompt = BASH_PROMPT
        if environment is None:
            environment = { "PS1": BASH_PROMPT }
        else:
            environment["PS1"] = BASH_PROMPT

    test_num_width = numDigits(len(test_suite["tests"]))
    output_file_name_root = f"{stringToFileName(test_suite['name'])}-{test_num:0>{test_num_width}}"
    expected_output_file = os.path.join(TEST_RESULTS_DIR, "raw", output_file_name_root + "-expected.tmp")
    actual_output_file = os.path.join(TEST_RESULTS_DIR, "raw", output_file_name_root + "-actual.tmp")
    results_output_file = os.path.join(TEST_RESULTS_DIR, output_file_name_root + "-results.tmp")
    valgrind_log_file = os.path.join(TEST_RESULTS_DIR, output_file_name_root + "-valgrd.tmp")
    valgrind_opts += f" --log-file={valgrind_log_file}"

    with contextlib.closing(MultiWriter(results_output_file, verbose_output)) as out:
        columns, _ = shutil.get_terminal_size()
        out.write('=' * columns + "\n")
        out.write(f"== Test {test_num}: {test_spec['name']}\n")
        out.write(f"== {test_spec['description']}\n")
        out.write("Running test...\n")

        with open(output_file) as f:
            expected_output = expandTemplate(f.read())
        actual_output, outcome = executeCommand(command, input_file, prompt, timeout, environment, use_valgrind, valgrind_opts)
        with open(expected_output_file, "w") as f:
            f.write(expected_output)
        with open(actual_output_file, "w") as f:
            f.write(actual_output)
        out.write(f"Expected output is in file '{expected_output_file}'\n")
        out.write(f"Actual output is in file '{actual_output_file}'\n")

        test_result = {
            "name": test_spec["name"],
            "index": test_num,
            "max_score": max_score,
        }

        if outcome is CommandOutcome.TIMED_OUT:
            test_result["score"] = 0
            test_result["outcome"] = f"Timed Out -> Results in {results_output_file}"
            test_result["output"] = f"Error: TIMED OUT. Output:\n{actual_output}"
            if not json_output:
                out.write("Error: TIMED OUT. Output:\n")
                out.write(actual_output + "\n")

        elif outcome is CommandOutcome.SEG_FAULT:
            test_result["score"] = 0
            test_result["outcome"] = f"Segmentation Fault -> Results in {results_output_file}"
            test_result["output"] = f"Error: SEGMENTATION FAULT. Output:\n{actual_output}"
            if not json_output:
                out.write("Error: SEGMENTATION FAULT. Output:\n")
                out.write(actual_output + "\n")

        elif outcome is CommandOutcome.VALGRIND_FAIL:
            test_result["score"] = 0
            test_result["outcome"] = f"Valgrind Failure -> Results in {results_output_file}"
            test_result["output"] = f"Error: VALGRIND CHECK FAILED. Output:\n{actual_output}"
            if not json_output:
                out.write("Error: VALGRIND CHECK FAILED. Output:\n")
                out.write(actual_output + "\n")
                out.write(f"\n== Valgrind Results (from '{valgrind_log_file}')\n")
                with open(valgrind_log_file) as f:
                    out.write(f.read())

        elif outcome is CommandOutcome.COMPLETED:
            output_match, diff = compareOutput(expected_output, actual_output)
            if output_match:
                test_result["score"] = max_score
                test_result["outcome"] = "Passed"
                if not json_output:
                    out.write("Test PASSED\n")
            else:
                test_result["score"] = 0
                test_result["outcome"] = f"Failed -> Results in {results_output_file}"
                test_result["output"] = diff
                if not json_output:
                    out.write("Test FAILED\n")
                    out.write(diff + "\n")

        else: # Should never happen
            raise ValueError(f"Unknown Command Outcome {outcome}")

    if not verbose_output and not json_output:
        printTestSummaryLine(test_result, test_num_width)
    return test_result


# Parses value of the '-n' command line argument, specifies which subset of
# tests within suite to run by those tests' indexes
# idx_spec: String containing test indexes
# num_tests: The total number of tests in the test suite
# Returns a list of indexes or 'None' if 'idx_spec' is invalid
def parseTestIndexes(idx_spec, num_tests):
    # Index range in the form "x-y"
    if "-" in idx_spec:
        tokens = re.split(r'\s*\-\s*', idx_spec)
        if len(tokens) != 2:
            return None
        try:
            start = int(tokens[0])
            end = int(tokens[1])
        except ValueError:
            return None
        if start <= 0 or start > num_tests:
            return None
        if end > num_tests:
            end = num_tests
        return list(range(start, end+1))

    # Index set in the form "a, b, c, d"
    elif "," in idx_spec:
        tokens = re.split(r'\s*\,\s*', idx_spec)
        try:
            indexes = [ int(x) for x in tokens ]
        except ValueError:
            return None
        if len(set(indexes)) != len(indexes):
            # Duplicates present
            return None
        elif any([ i <= 0 or i > num_tests for i in indexes ]):
            # At least one index out of valid range
            return None
        return indexes

    # Assume a single test number is specified
    try:
        index = int(idx_spec)
    except ValueError:
        return None
    if index <= 0 or index > num_tests:
        return None
    return [index]


if __name__ == '__main__':
    parser = argparse.ArgumentParser("testius")
    parser.add_argument("test_file")
    parser.add_argument("-j", "--json", action="store_true")
    parser.add_argument("-n", "--numbers")
    parser.add_argument("-v", "--verbose", action="store_true")
    arguments = parser.parse_args()

    if arguments.json and arguments.verbose:
        print("Error: Cannot specify both JSON and verbose output modes")
        sys.exit(1)

    if not os.path.isfile(arguments.test_file):
        print(f'Error: "{arguments.test_file}" does not exist or is not a valid file')
        sys.exit(1)

    with open(arguments.test_file) as test_file:
        try:
            test_suite = json.load(test_file)
        except:
            print("Error: Specified file must be valid JSON")
            sys.exit(1)

    if not isValidTestSuite(test_suite):
        print("Error: Invalid test suite file")
        sys.exit(1)

    if any([ "use_valgrind" in test for test in test_suite["tests"] ]):
        if shutil.which("valgrind") is None:
            print("Error: These tests require the 'valgrind' program which is not currently installed")
            print("Install valgrind and then re-run these tests")
            sys.exit(1)

    test_indexes = list(range(1, len(test_suite["tests"]) + 1))
    if arguments.numbers is not None:
        specified_tests = parseTestIndexes(arguments.numbers, len(test_suite["tests"]))
        if specified_tests is None:
            print(f'Error: Invalid Test Index Specification "{arguments.numbers}"')
            sys.exit(1)
        else:
            test_indexes = specified_tests

    if os.path.exists(TEST_RESULTS_DIR):
        shutil.rmtree(TEST_RESULTS_DIR)
    os.mkdir(TEST_RESULTS_DIR)
    os.mkdir(os.path.join(TEST_RESULTS_DIR, "raw"))

    num_tests_to_run = len(test_indexes)
    if not arguments.json:
        print(f"== {test_suite['name']}")
        print(f"== Running {num_tests_to_run}/{len(test_suite['tests'])} tests")

    test_results = { "tests": [] }
    for idx in test_indexes:
        try:
            test_results["tests"].append(runTest(idx, test_suite, arguments.json, arguments.verbose))
        except KeyboardInterrupt:
            # Stop tests, but still print out summary
            break

    if arguments.json:
        print(json.dumps(test_results))
    else:
        num_tests_passed = 0
        total_score = 0
        total_max_score = 0
        for result in test_results["tests"]:
            total_score += result["score"]
            total_max_score += result["max_score"]
            if result["score"] > 0:
                num_tests_passed += 1

        if arguments.verbose:
            columns, _ = shutil.get_terminal_size()
            print('=' * columns)
            print("== Summary of Results")
            max_test_digits = numDigits(len(test_suite["tests"]))
            for result in test_results["tests"]:
                printTestSummaryLine(result, max_test_digits)

        print()
        num_tests_ran = len(test_results["tests"])
        print(f"Ran {num_tests_ran}/{num_tests_to_run} Requested Tests")
        print(f"Passed {num_tests_passed}/{num_tests_ran} Tests")
        print(f"Total Score: {total_score}/{total_max_score}")
